--- File Tree Structure ---
|-- algo.py
|-- backtest.py
|-- benchmark.py
|-- build_bars.py
|-- config.py
|-- data.py
|-- metrics.py
|-- quick.py

// --- File: algo.py ---

```python
"""
algo.py - Fractal Edge Strategy
Consumes 'Fractal Delta Bars' generated by build_bars.py.
"""
import sys

# NO complex calculations here. 
# Math was done in C/Rust speeds during build.

def decide(bar):
    """
    Input: Tuple (ts_start, ts_end, o, h, l, c, vol, delta, eff, impact)
    Output: Signal (-1, 0, 1)
    """
    ts_start, ts_end, o, h, l, c, vol, delta, eff, impact = bar
    
    # --- LOGIC: The "Absorption Hunter" ---
    
    # 1. Define Regime
    # High Efficiency (> 0.6) = Trending / Momentum
    # Low Efficiency (< 0.3) = Choppy / Mean Reversion / Absorption
    
    signal = 0
    
    if eff > 0.6:
        # MOMENTUM REGIME
        # If Delta agrees with Price Direction, Follow it.
        # But check Impact: We want HIGH impact (price moving easily)
        
        price_move = c - o
        
        if delta > 0 and price_move > 0:
            # Bullish Flow + Bullish Price + Efficient Path
            signal = 1
        elif delta < 0 and price_move < 0:
            signal = -1
            
    elif eff < 0.25:
        # ABSORPTION REGIME (The "Edge")
        # Heavy Delta but Price NOT moving efficiently (Grinding)
        # This often signals a reversal or iceberg absorption.
        
        # If Huge Buying (Delta > 0) but Price NOT going up efficiently
        # -> Sellers are absorbing. FADE IT.
        if delta > 0:
            signal = -1 # Sell into the absorption
            
        # If Huge Selling (Delta < 0) but Price holding
        # -> Buyers are absorbing. BUY IT.
        elif delta < 0:
            signal = 1
            
    return signal
```

// --- End File: algo.py ---

// --- File: backtest.py ---

```python
"""
backtest.py - Runs on Pre-Built Bars
"""
import sys
import os
import time
import struct
from concurrent.futures import ThreadPoolExecutor

if sys._is_gil_enabled():
    raise RuntimeError("Run with -X gil=0")

import config
import algo
import metrics
import data

def worker_bars(shard_idx, days_to_process):
    trades = []
    BAR_SZ = config.BAR_SIZE
    iter_bars = config.BAR_STRUCT.iter_unpack
    
    position = 0
    entry_px = 0.0
    entry_ts = 0.0
    
    # Metadata for metrics
    entry_k1 = 0.0
    entry_k3 = 0.0

    for (raw_path, _, _, date_str) in days_to_process:
        bar_path = raw_path.replace("data.quantdev", "data.bars")
        if not os.path.exists(bar_path): continue
            
        try:
            with open(bar_path, "rb") as f:
                blob = f.read()
            
            for bar in iter_bars(blob):
                # Unpack: ts_start, ts_end, o, h, l, c, vol, delta, eff, impact
                ts_end = bar[1]
                c = bar[5]
                
                # Exit
                if position != 0:
                    roi_bps = ((c - entry_px) / entry_px) * 10000.0 * position
                    
                    # Simple SL/TP logic for test
                    if roi_bps > 40 or roi_bps < -15:
                        pnl = roi_bps - config.COST_BASIS_BPS
                        trades.append({
                            "entry_ts": entry_ts,
                            "exit_ts": ts_end,
                            "net_pnl_bps": pnl,
                            "side": position,
                            "k1": entry_k1,
                            "k3": entry_k3
                        })
                        position = 0
                
                # Entry
                sig = algo.decide(bar)
                if position == 0 and sig != 0:
                    position = sig
                    entry_px = c
                    entry_ts = ts_end
                    # Capture Regime Stats for Metrics
                    # bar[7] = delta, bar[8] = eff
                    entry_k1 = bar[8] # Proxy efficiency as K1
                    entry_k3 = bar[9] # Proxy impact as K3
                    
        except Exception: continue
        
    return trades

def print_scorecard(sc):
    """Pretty prints the dictionary."""
    print("\n" + "="*60)
    print("MASTER SCORECARD SUMMARY")
    print("="*60)
    
    s = sc["strategy_returns"]
    print(f"[STRATEGY]")
    print(f"  Total Trades:     {s['total_trades']}")
    print(f"  Net PnL:          {s['net_pnl_bps']:.2f} bps")
    print(f"  Sharpe (Ann):     {s['sharpe_annualized']:.2f}")
    print(f"  Sortino (Ann):    {s['sortino_ratio']:.2f}")
    print(f"  Max Drawdown:     {s['max_drawdown_bps']:.2f} bps")
    print(f"  Recovery Factor:  {s['recovery_factor']:.2f}")

    t = sc["trade_level"]
    print(f"\n[TRADES]")
    print(f"  Win Rate:         {t['win_rate_pct']:.2f}%")
    print(f"  Avg Win:          {t['avg_win_bps']:.2f} bps")
    print(f"  Avg Loss:         {t['avg_loss_bps']:.2f} bps")
    print(f"  Profit Factor:    {t['profit_factor']:.2f}")
    print(f"  SQN:              {t['sqn']:.2f}")

    r = sc["regime_conditional"]
    print(f"\n[REGIMES]")
    print(f"  Low Vol PnL:      {r['avg_pnl_low_vol']:.2f} bps (n={r['count_low_vol']})")
    print(f"  High Vol PnL:     {r['avg_pnl_high_vol']:.2f} bps (n={r['count_high_vol']})")

    rob = sc["robustness_overfitting"]
    print(f"\n[ROBUSTNESS]")
    print(f"  IS Sharpe (1st Half):  {rob['sharpe_is_first_half']:.2f}")
    print(f"  OOS Sharpe (2nd Half): {rob['sharpe_oos_second_half']:.2f}")
    print("="*60 + "\n")

def run():
    print("--- Backtesting with Full Metrics ---")
    years = data.discover_years(config.SYMBOL)
    all_jobs = data.scan_jobs(config.SYMBOL, years=years)
    
    w = config.WORKERS
    n = len(all_jobs)
    k, m = divmod(n, w)
    shards = [all_jobs[i*k+min(i,m):(i+1)*k+min(i+1,m)] for i in range(w)]
    
    t0 = time.perf_counter()
    trades = []
    
    with ThreadPoolExecutor(max_workers=w) as ex:
        futs = [ex.submit(worker_bars, i, s) for i, s in enumerate(shards)]
        for f in futs:
            trades.extend(f.result())
            
    print(f"Simulation: {time.perf_counter()-t0:.2f}s")
    
    if trades:
        scorecard = metrics.generate_scorecard(trades)
        print_scorecard(scorecard)
    else:
        print("No trades generated.")

if __name__ == "__main__":
    run()
```

// --- End File: backtest.py ---

// --- File: benchmark.py ---

```python
import sys
import time
import array
from concurrent.futures import ThreadPoolExecutor

if sys._is_gil_enabled():
    raise RuntimeError("Performance Critical: Must run on Python 3.14t (Free-Threaded)")

CPU_THREADS = 24
DATA_SIZE = 50_000_000  # 50 Million ticks

# Setup Data
prices = array.array('d', [100.0 + (i % 100) * 0.05 for i in range(DATA_SIZE)])
signals = array.array('b', [(i % 3) - 1 for i in range(DATA_SIZE)]) # -1, 0, 1 (Sell, Hold, Buy)

print(f"Stateful Benchmark: {DATA_SIZE:,} ticks on {CPU_THREADS} threads")

def backtest_worker(start, end, initial_cash):
    """
    Complex Stateful Logic:
    - Maintains 'cash' and 'position' variables.
    - Loops cannot be vectorized easily by Polars without 'scan'.
    - 3.14t handles this naturally in parallel.
    """
    local_prices = prices
    local_signals = signals
    
    cash = initial_cash
    position = 0.0
    
    # The Loop that kills Pandas/Polars performance
    for i in range(start, end):
        price = local_prices[i]
        sig = local_signals[i]
        
        if sig == 1 and cash > price: # Buy
            position += 1
            cash -= price
        elif sig == -1 and position > 0: # Sell
            position -= 1
            cash += price
            
    return cash + (position * local_prices[end-1])

print("-" * 60)
print("Running Stateful Iterative Backtest (The 'Loop' Killer)...")

start_time = time.perf_counter()

chunk_size = DATA_SIZE // CPU_THREADS
futures = []

with ThreadPoolExecutor(max_workers=CPU_THREADS) as executor:
    # Distribute chunks
    # Note: Real backtests need state passing between chunks, 
    # but for raw throughput testing, we treat them as independent strategy shards.
    for i in range(CPU_THREADS):
        s = i * chunk_size
        e = s + chunk_size if i < CPU_THREADS - 1 else DATA_SIZE
        futures.append(executor.submit(backtest_worker, s, e, 100000.0))

final_values = [f.result() for f in futures]

duration = time.perf_counter() - start_time
tps = DATA_SIZE / duration

print(f"Time: {duration:.4f}s")
print(f"TPS:  {tps:,.0f} Events/Sec")
print("-" * 60)
print("INTERPRETATION:")
print("This loop contains if/elif logic and state updates per row.")
print("To do this in Polars requires 'expr.map_elements' (slow) or Rust.")
print("Python 3.14t does it naturally at C-speed.")
```

// --- End File: benchmark.py ---

// --- File: build_bars.py ---

```python
"""
build_bars.py - Feature Engineering Engine
Target: Python 3.14t (Free-Threaded)
Reads: AGG3 Raw Ticks (Zstd)
Writes: Fractal Delta Bars (Binary)

Logic:
aggregates ticks until Abs(CumulativeDelta) >= DELTA_THRESHOLD.
Calculates Fractal Efficiency and Price Impact on the fly.
"""

import sys
import os
import struct
import math
from concurrent.futures import ThreadPoolExecutor

if sys._is_gil_enabled():
    raise RuntimeError("Run with -X gil=0")

import config
import data  # We use data.py only for discovery

try:
    import compression.zstd as zstd
except ImportError:
    print("[FATAL] compression.zstd missing")
    sys.exit(1)

def process_day_to_bars(job):
    """
    Reads 1 day of raw ticks, outputs 1 file of bars.
    """
    src_path, off, ln, date_str = job
    
    # Output Path: data/BTCUSDT/YYYY/MM/data.bars
    # We append to a month-level bar file or create a day-level one?
    # For speed, let's create day-level bar files next to raw data
    dst_path = src_path.replace("data.quantdev", "data.bars")
    # If the file exists and is essentially the same time as raw, skip? 
    # For now, overwrite to ensure fresh algo logic.

    # 1. Read & Decompress Raw
    try:
        with open(src_path, "rb") as f:
            f.seek(off)
            blob = f.read(ln)
        raw_bytes = zstd.decompress(blob)
    except Exception as e:
        return f"ErrRead: {e}"

    # 2. Iterators
    iter_rows = config.AGG_ROW_STRUCT.iter_unpack
    raw_data = raw_bytes[config.AGG_HDR_SIZE:] # Skip Header
    
    # 3. Bar Accumulators
    bar_rows = bytearray()
    
    # State
    b_open = 0.0
    b_high = -1.0
    b_low = 1e9
    b_vol = 0.0
    b_buy_vol = 0.0
    b_sell_vol = 0.0
    b_ts_start = 0.0
    
    # Fractal Math State
    path_length = 0.0 # Sum of absolute price moves
    net_change = 0.0  # Open to Close
    
    TARGET = config.DELTA_THRESHOLD * config.QT_SCALE # Convert BTC to scaled units
    
    px_scale_inv = 1.0 / config.PX_SCALE
    qt_scale_inv = 1.0 / config.QT_SCALE
    
    last_px = 0.0
    
    for r in iter_rows(raw_data):
        # r: id, px, qt, fi, cnt, flags, ts, pad
        px_raw = r[1]
        qt_raw = r[2]
        ts = r[6]
        
        # Init Bar if empty
        if b_open == 0.0:
            b_open = px_raw
            b_high = px_raw
            b_low = px_raw
            b_ts_start = ts
            last_px = px_raw
            # path_length reset
            path_length = 0.0

        # High/Low
        if px_raw > b_high: b_high = px_raw
        if px_raw < b_low: b_low = px_raw
        
        # Path Length (for Efficiency)
        if last_px > 0:
            path_length += abs(px_raw - last_px)
        last_px = px_raw
        
        # Volume / Delta
        b_vol += qt_raw
        # flags: 1 = Maker is Buyer (so Taker is Seller) -> Sell
        # flags: 0 = Maker is Seller (so Taker is Buyer) -> Buy
        is_sell = (r[5] & 1)
        if is_sell:
            b_sell_vol += qt_raw
        else:
            b_buy_vol += qt_raw
            
        # Check Trigger: Absolute Net Delta
        net_delta_raw = b_buy_vol - b_sell_vol
        
        if abs(net_delta_raw) >= TARGET:
            # --- SEAL BAR ---
            b_close = px_raw
            b_ts_end = ts
            
            # 1. Prices
            o = b_open * px_scale_inv
            h = b_high * px_scale_inv
            l = b_low * px_scale_inv
            c = b_close * px_scale_inv
            
            # 2. Volume/Delta
            v = b_vol * qt_scale_inv
            d = net_delta_raw * qt_scale_inv
            
            # 3. Novel Metrics
            
            # Efficiency: (Net Move) / (Total Path)
            # 1.0 = Straight line, 0.0 = Noise
            price_dist = abs(b_close - b_open)
            eff = 0.0
            if path_length > 0:
                eff = price_dist / path_length
            
            # Impact: Price Move per Unit of Delta
            # High Impact = Low Liquidity
            # Low Impact = High Liquidity (Absorption)
            impact = 0.0
            if abs(d) > 1e-9:
                impact = (c - o) / d
                
            # Pack
            # ts_start, ts_end, o, h, l, c, vol, delta, eff, impact
            bar_rows.extend(config.BAR_STRUCT.pack(
                float(b_ts_start), float(b_ts_end),
                o, h, l, c, v, d, eff, impact
            ))
            
            # Reset
            b_open = 0.0
            b_vol = 0.0
            b_buy_vol = 0.0
            b_sell_vol = 0.0
            path_length = 0.0

    # Write Result
    if len(bar_rows) > 0:
        # We write a standalone .bars file for this day
        # Format: Just the structs appended
        try:
            # Overwrite mode
            with open(dst_path, "wb") as f_out:
                f_out.write(bar_rows)
            return "ok"
        except Exception:
            return "err_write"
            
    return "empty"

def run_builder():
    print(f"--- Building Fractal Delta Bars (Thresh: {config.DELTA_THRESHOLD} BTC) ---")
    
    # 1. Discover Data
    years = data.discover_years(config.SYMBOL)
    if not years:
        print("No raw data found.")
        return

    # 2. Generate Jobs
    # We can reuse data.scan_jobs, but we need the output paths to map to .bars
    raw_jobs = data.scan_jobs(config.SYMBOL, years=years)
    print(f"[Builder] Processing {len(raw_jobs)} days of ticks...")
    
    t0 = math.inf # just for imports, use time.perf_counter really
    import time
    t0 = time.perf_counter()
    
    # 3. Parallel Execution
    stats = {"ok": 0, "empty": 0, "err": 0}
    
    with ThreadPoolExecutor(max_workers=config.WORKERS) as pool:
        # raw_jobs is list of (path, off, ln, date)
        futures = [pool.submit(process_day_to_bars, j) for j in raw_jobs]
        
        for f in futures:
            res = f.result()
            if res == "ok": stats["ok"] += 1
            elif res == "empty": stats["empty"] += 1
            else: stats["err"] += 1
            
    print(f"[Builder] Done in {time.perf_counter()-t0:.2f}s. Stats: {stats}")

if __name__ == "__main__":
    run_builder()
```

// --- End File: build_bars.py ---

// --- File: config.py ---

```python
"""
config.py
Shared Configuration & Schemas.
"""
import struct

# --- PATHS ---
BASE_DIR = "data"
SYMBOL = "BTCUSDT"

# --- RAW DATA (AGG3) - Input for Builder ---
AGG_ROW_STRUCT = struct.Struct("<QQQQIHq2x")
AGG_HDR_STRUCT = struct.Struct("<4sBBHQqq16x")
AGG_HDR_SIZE = 48
AGG_ROW_SIZE = 48
PX_SCALE = 100_000_000.0
QT_SCALE = 100_000_000.0

# --- NOVEL BARS (OUTPUT of Builder) ---
# Format:
# 1. ts_start (d): Unix timestamp float
# 2. ts_end   (d): Unix timestamp float
# 3. open     (d): Price
# 4. high     (d): Price
# 5. low      (d): Price
# 6. close    (d): Price
# 7. vol      (d): Total Volume
# 8. delta    (d): Net Delta (Buy - Sell)
# 9. effic    (d): Efficiency Ratio (0.0 to 1.0) - Absorption Proxy
# 10. impact  (d): Price Impact (Price Change / Delta) - Liquidity Proxy
BAR_STRUCT = struct.Struct("<dddddddddd")
BAR_SIZE = BAR_STRUCT.size  # 80 bytes per bar

# --- SETTINGS ---
DELTA_THRESHOLD = 50.0  # BTC Net Delta to trigger a new bar
WORKERS = 24            # Ryzen 7900X
```

// --- End File: config.py ---

// --- File: data.py ---

```python
#!/usr/bin/env python3.14t
"""
data.py (v3.3 - Free-Threaded, AGG3, Index v1 + Checksums)

Target runtime:
- Python 3.14t, GIL disabled (-X gil=0)
- Windows 11
- AMD Ryzen 9 7900X (12C / 24T)

Features:
- AGG3 row format:
    * 48 bytes, fixed width, 8-byte aligned row size
    * trade_id(Q), px_scaled(Q), qty_scaled(Q), first_id(Q),
      trade_count(I, 32-bit), flags(H), ts_ms(q), pad(2x)
- Header per day with magic/version/day/zstd_level/row_count/min_ts/max_ts.
- Index v1 with header + 64-bit checksum of uncompressed day blob.
- Backward compatibility with legacy v0 index.
- Free-threaded Python 3.14t only, using compression.zstd.
"""

import os
import time
import struct
import datetime as dt
import http.client
import ssl
import signal
import zipfile
import hashlib
import sys
import threading
from io import BytesIO, TextIOWrapper
from csv import reader as csv_reader
from concurrent.futures import ThreadPoolExecutor, as_completed

# ---------------------------------------------------------------------------
# 0. Mandatory 3.14t / Free-threaded boilerplate
# ---------------------------------------------------------------------------

if sys._is_gil_enabled():
    raise RuntimeError("Performance Critical: Must run on Python 3.14t with -X gil=0")

CPU_THREADS = 24

try:
    import compression.zstd as zstd
except ImportError as exc:
    raise RuntimeError("Native 'compression.zstd' (PEP 784) is required.") from exc

# ---------------------------------------------------------------------------
# 1. Configuration
# ---------------------------------------------------------------------------

CONFIG: dict[str, object] = {
    "SYMBOL": "BTCUSDT",
    "BASE_DIR": "data",
    
    # Binance Data Source
    "HOST_DATA": "data.binance.vision",
    "S3_PREFIX": "data/futures/um",
    "DATASET": "aggTrades",

    "FALLBACK_DATE": dt.date(2020, 1, 1),
    
    "WORKERS": CPU_THREADS,
    "ROWS_PER_CHUNK": 50_000,
    
    "TIMEOUT": 10,
    "RETRIES": 5,
    "BACKOFF": 0.5,
    "VERIFY_SSL": True,
    "USER_AGENT": "QuantEngine/3.14t data.py",
    
    "ZSTD_LEVEL": 3,
}

# ---------------------------------------------------------------------------
# 2. Binary Schema (AGG3)
# ---------------------------------------------------------------------------

PX_SCALE = 100_000_000
QT_SCALE = 100_000_000

# Row: trade_id(Q), px(Q), qt(Q), fi(Q), count(I), flags(H), ts(q), pad(2x)
AGG_ROW_STRUCT = struct.Struct("<QQQQIHq2x")
AGG_ROW_SIZE = AGG_ROW_STRUCT.size
assert AGG_ROW_SIZE == 48

# Header: magic(4s), ver(B), day(B), z_lvl(H), count(Q), min_ts(q), max_ts(q), pad(16x)
AGG_HDR_STRUCT = struct.Struct("<4sBBHQqq16x")
AGG_HDR_MAGIC = b"AGG3"
assert AGG_HDR_STRUCT.size == 48

# Index Header: magic(4s), ver(I), count(Q)
INDEX_MAGIC = b"QIDX"
INDEX_VERSION = 1
INDEX_HDR_STRUCT = struct.Struct("<4sIQ")
INDEX_HDR_SIZE = INDEX_HDR_STRUCT.size

# Index Rows
INDEX_ROW_STRUCT_V0 = struct.Struct("<HQQ")    # Legacy
INDEX_ROW_STRUCT_V1 = struct.Struct("<HQQQ")   # v1 with checksum
INDEX_ROW_SIZE_V0 = INDEX_ROW_STRUCT_V0.size
INDEX_ROW_SIZE_V1 = INDEX_ROW_STRUCT_V1.size

FLAG_IS_BUYER_MAKER = 1 << 0

# ---------------------------------------------------------------------------
# 3. Global State & Locks
# ---------------------------------------------------------------------------

_thread_local = threading.local()
_stop_event = threading.Event()

# Fine-grained locking: (Year, Month) -> Lock
_month_locks: dict[tuple[int, int], threading.Lock] = {}
_guard = threading.Lock()

_ssl_context = ssl.create_default_context()
if not CONFIG["VERIFY_SSL"]:
    _ssl_context.check_hostname = False
    _ssl_context.verify_mode = ssl.CERT_NONE


def _get_month_lock(year: int, month: int) -> threading.Lock:
    key = (year, month)
    # Optimistic check
    if key in _month_locks:
        return _month_locks[key]
    with _guard:
        # Double-check
        if key not in _month_locks:
            _month_locks[key] = threading.Lock()
        return _month_locks[key]


def _ensure_paths(year: int, month: int) -> tuple[str, str, str]:
    base_dir = str(CONFIG["BASE_DIR"])
    symbol = str(CONFIG["SYMBOL"])
    dir_path = os.path.join(base_dir, symbol, f"{year:04d}", f"{month:02d}")
    os.makedirs(dir_path, exist_ok=True)
    data_path = os.path.join(dir_path, "data.quantdev")
    index_path = os.path.join(dir_path, "index.quantdev")
    return dir_path, data_path, index_path

# ---------------------------------------------------------------------------
# 3a. Index & Integrity Helpers
# ---------------------------------------------------------------------------

def _checksum64(data: bytes) -> int:
    """64-bit checksum using blake2b (stdlib)."""
    h = hashlib.blake2b(digest_size=8)
    h.update(data)
    return int.from_bytes(h.digest(), "little")


def _detect_index_layout(index_path: str) -> int:
    """Returns 0 for legacy/missing, 1 for v1 headered."""
    try:
        size = os.path.getsize(index_path)
        if size < INDEX_HDR_SIZE:
            return 0
        with open(index_path, "rb") as f:
            head = f.read(INDEX_HDR_SIZE)
        if len(head) != INDEX_HDR_SIZE:
            return 0
        magic, version, _ = INDEX_HDR_STRUCT.unpack(head)
        if magic == INDEX_MAGIC and version == INDEX_VERSION:
            return 1
        return 0
    except OSError:
        return 0


def _init_index_v1_if_needed(index_path: str) -> None:
    """Create v1 index header if file missing or empty."""
    try:
        if not os.path.exists(index_path) or os.path.getsize(index_path) == 0:
            with open(index_path, "wb") as f:
                f.write(INDEX_HDR_STRUCT.pack(INDEX_MAGIC, INDEX_VERSION, 0))
    except OSError:
        pass


def _increment_index_v1_count(index_path: str, delta: int) -> None:
    """Update record_count in v1 header by +delta."""
    try:
        with open(index_path, "r+b") as f:
            head = f.read(INDEX_HDR_SIZE)
            if len(head) != INDEX_HDR_SIZE:
                return
            magic, version, count = INDEX_HDR_STRUCT.unpack(head)
            if magic == INDEX_MAGIC and version == INDEX_VERSION:
                count += delta
                f.seek(0)
                f.write(INDEX_HDR_STRUCT.pack(magic, version, count))
                f.flush()
                os.fsync(f.fileno())
    except OSError:
        pass


def _is_day_indexed(year: int, month: int, day: int) -> bool:
    """Check if index contains a valid entry for (year, month, day)."""
    _, data_path, index_path = _ensure_paths(year, month)
    if not os.path.exists(index_path) or not os.path.exists(data_path):
        return False

    try:
        data_size = os.path.getsize(data_path)
    except OSError:
        return False

    layout = _detect_index_layout(index_path)
    row_struct = INDEX_ROW_STRUCT_V1 if layout == 1 else INDEX_ROW_STRUCT_V0
    row_size = INDEX_ROW_SIZE_V1 if layout == 1 else INDEX_ROW_SIZE_V0
    start_offset = INDEX_HDR_SIZE if layout == 1 else 0

    try:
        with open(index_path, "rb") as f:
            f.seek(start_offset)
            while True:
                raw = f.read(row_size)
                if len(raw) < row_size:
                    break
                
                if layout == 1:
                    d, off, length, _ = row_struct.unpack(raw)
                else:
                    d, off, length = row_struct.unpack(raw)
                
                if d == day:
                    if (off + length) <= data_size:
                        return True
                    return False
    except OSError:
        return False
    return False

# ---------------------------------------------------------------------------
# 4. Networking
# ---------------------------------------------------------------------------

def _get_connection(host: str) -> http.client.HTTPSConnection:
    conns = getattr(_thread_local, "conns", None)
    if conns is None:
        conns = {}
        _thread_local.conns = conns
    
    conn = conns.get(host)
    if conn is None or conn.sock is None:
        conn = http.client.HTTPSConnection(
            host, context=_ssl_context, timeout=float(CONFIG["TIMEOUT"])
        )
        conns[host] = conn
    return conn


def _close_connection(host: str) -> None:
    conns = getattr(_thread_local, "conns", None)
    if conns:
        conn = conns.pop(host, None)
        if conn:
            try:
                conn.close()
            except Exception:
                pass


def _http_request(host: str, method: str, path: str) -> bytes | None:
    headers = {
        "User-Agent": str(CONFIG["USER_AGENT"]),
        "Accept-Encoding": "identity",
    }
    retries = int(CONFIG["RETRIES"])
    backoff = float(CONFIG["BACKOFF"])

    for attempt in range(1, retries + 1):
        if _stop_event.is_set():
            return None
        
        try:
            conn = _get_connection(host)
            conn.request(method, path, headers=headers)
            resp = conn.getresponse()
            data = resp.read()
            resp.close()

            if resp.status == 200:
                return data
            if resp.status == 404:
                return None

        except (http.client.HTTPException, OSError, ssl.SSLError):
            _close_connection(host)

        if attempt < retries:
            time.sleep(backoff * (2 ** (attempt - 1)))

    return None

# ---------------------------------------------------------------------------
# 5. Zip -> AGG3
# ---------------------------------------------------------------------------

def _process_zip_to_agg3(day: dt.date, zip_bytes: bytes) -> bytes | None:
    buf_size = AGG_ROW_SIZE * int(CONFIG["ROWS_PER_CHUNK"])
    
    # Thread-local buffer reuse to minimize allocs
    buf = getattr(_thread_local, "buf", None)
    if buf is None or len(buf) != buf_size:
        buf = bytearray(buf_size)
        _thread_local.buf = buf
    
    view = memoryview(buf)
    pack_into = AGG_ROW_STRUCT.pack_into
    chunks: list[bytes] = []
    
    c_min = 2**63 - 1
    c_max = -2**63
    total_count = 0

    try:
        with zipfile.ZipFile(BytesIO(zip_bytes)) as zf:
            csv_names = [n for n in zf.namelist() if n.endswith(".csv")]
            if not csv_names:
                return None
            
            with zf.open(csv_names[0], "r") as f_in:
                wrapper = TextIOWrapper(f_in, encoding="utf-8", newline="")
                reader = csv_reader(wrapper)
                
                headers = next(reader, None)
                if not headers:
                    return None
                
                # Normalize headers
                h_map = {h.replace("_", "").lower().strip(): i for i, h in enumerate(headers)}
                
                idx_id = h_map.get("aggtradeid", h_map.get("id", 0))
                idx_px = h_map.get("price", 1)
                idx_qt = h_map.get("quantity", 2)
                idx_fi = h_map.get("firsttradeid", 3)
                idx_li = h_map.get("lasttradeid", 4)
                idx_ts = h_map.get("transacttime", 5)
                idx_bm = h_map.get("isbuyermaker", 6)
                
                offset = 0
                
                for row in reader:
                    if not row:
                        continue
                    try:
                        ts = int(row[idx_ts])
                        if ts < c_min: c_min = ts
                        if ts > c_max: c_max = ts
                        
                        fi = int(row[idx_fi])
                        li = int(row[idx_li])
                        cnt = li - fi + 1
                        if cnt <= 0: continue
                        if cnt > 0xFFFFFFFF: cnt = 0xFFFFFFFF
                        
                        # Use round to correct floating point drift before int cast
                        px = int(round(float(row[idx_px]) * PX_SCALE))
                        qt = int(round(float(row[idx_qt]) * QT_SCALE))
                        
                        is_maker = row[idx_bm].lower() in ("true", "1", "t")
                        flags = FLAG_IS_BUYER_MAKER if is_maker else 0
                        
                        pack_into(view, offset, int(row[idx_id]), px, qt, fi, cnt, flags, ts)
                        offset += AGG_ROW_SIZE
                        total_count += 1
                        
                        if offset >= buf_size:
                            chunks.append(view[:offset].tobytes())
                            offset = 0
                            
                    except (ValueError, IndexError):
                        continue
                
                if offset > 0:
                    chunks.append(view[:offset].tobytes())
                    
    except Exception:
        return None
    
    if total_count == 0:
        return b""
        
    hdr = AGG_HDR_STRUCT.pack(
        AGG_HDR_MAGIC, 1, day.day, int(CONFIG["ZSTD_LEVEL"]), total_count, c_min, c_max
    )
    return hdr + b"".join(chunks)

# ---------------------------------------------------------------------------
# 6. Per-day worker
# ---------------------------------------------------------------------------

def _process_day(day: dt.date) -> str:
    if _stop_event.is_set():
        return "stop"
    
    y, m, d = day.year, day.month, day.day
    
    # 1. Check index (fast path)
    if _is_day_indexed(y, m, d):
        return "skip"

    # 2. Download
    try:
        year_str = f"{y}"
        month_str = f"{m:02d}"
        day_str = f"{d:02d}"
        sym = str(CONFIG["SYMBOL"])
        ds = str(CONFIG["DATASET"])
        pfx = str(CONFIG["S3_PREFIX"])
        path = f"/{pfx}/daily/{ds}/{sym}/{sym}-{ds}-{year_str}-{month_str}-{day_str}.zip"
        
        zip_data = _http_request(str(CONFIG["HOST_DATA"]), "GET", path)
    except Exception:
        return "error"
        
    if _stop_event.is_set():
        return "stop"
    if zip_data is None:
        return "missing"
    
    # 3. Process ZIP -> AGG3
    agg_blob = _process_zip_to_agg3(day, zip_data)
    if agg_blob is None:
        return "error"
    if len(agg_blob) == 0:
        return "missing" # Empty CSV

    # 4. Checksum & Compress
    c_sum = _checksum64(agg_blob)
    try:
        c_blob = zstd.compress(agg_blob, level=int(CONFIG["ZSTD_LEVEL"]))
    except Exception:
        return "error"
        
    blob_len = len(c_blob)
    
    # 5. Write (Critical Section)
    lock = _get_month_lock(y, m)
    _, data_path, index_path = _ensure_paths(y, m)
    
    try:
        with lock:
            if _is_day_indexed(y, m, d):
                return "skip"

            if not os.path.exists(data_path):
                with open(data_path, "wb"): pass
            
            layout = _detect_index_layout(index_path)
            if layout == 0:
                _init_index_v1_if_needed(index_path)
                layout = _detect_index_layout(index_path)

            # Append Data
            with open(data_path, "ab") as f_data:
                start_offset = f_data.tell()
                f_data.write(c_blob)
                f_data.flush()
                os.fsync(f_data.fileno())

            # Append Index
            if layout == 1:
                row = INDEX_ROW_STRUCT_V1.pack(d, start_offset, blob_len, c_sum)
            else:
                row = INDEX_ROW_STRUCT_V0.pack(d, start_offset, blob_len)
                
            with open(index_path, "ab") as f_idx:
                f_idx.write(row)
                f_idx.flush()
                os.fsync(f_idx.fileno())
                
            if layout == 1:
                _increment_index_v1_count(index_path, 1)
        
        return "ok"
    except OSError:
        return "error"

def _worker_chunk(days: list[dt.date]) -> dict[str, int]:
    stats: dict[str, int] = {"ok": 0, "skip": 0, "missing": 0, "error": 0, "stop": 0}
    for day in days:
        if _stop_event.is_set():
            stats["stop"] += 1
            break
        res = _process_day(day)
        stats[res] = stats.get(res, 0) + 1
    return stats

# ---------------------------------------------------------------------------
# 7. Main
# ---------------------------------------------------------------------------

def _split_into_chunks(seq: list[dt.date], n: int) -> list[list[dt.date]]:
    k, m = divmod(len(seq), n)
    return [seq[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n)]

def main() -> None:
    signal.signal(signal.SIGINT, lambda s, f: _stop_event.set())
    
    print(f"--- data.py (3.3 / Free-Threaded) | Symbol: {CONFIG['SYMBOL']} ---")
    
    s_date = CONFIG["FALLBACK_DATE"]
    assert isinstance(s_date, dt.date)
    e_date = dt.datetime.now(dt.timezone.utc).date() - dt.timedelta(days=1)
    
    if e_date < s_date:
        print("Nothing to do.")
        return

    days = [s_date + dt.timedelta(days=i) for i in range((e_date - s_date).days + 1)]
    chunks = _split_into_chunks(days, CPU_THREADS)
    
    print(f"[job] {len(days)} days -> {CPU_THREADS} threads.")
    
    t0 = time.perf_counter()
    stats_total: dict[str, int] = {}
    
    with ThreadPoolExecutor(max_workers=CPU_THREADS) as ex:
        futs = [ex.submit(_worker_chunk, c) for c in chunks]
        
        done_cnt = 0
        for f in as_completed(futs):
            for k, v in f.result().items():
                stats_total[k] = stats_total.get(k, 0) + v
            done_cnt += 1
            print(f"[status] Chunks: {done_cnt}/{CPU_THREADS} | Stats: {stats_total}", end="\r")
            
    print(f"\n[done] {time.perf_counter() - t0:.2f}s | Final: {stats_total}")

if __name__ == "__main__":
    main()
```

// --- End File: data.py ---

// --- File: metrics.py ---

```python
"""
metrics.py
The Master Scorecard Engine.
Calculates "Max Data" trade statistics using Python Standard Library.
"""
import math
import statistics
import collections
from itertools import groupby

def _safe_mean(data):
    return statistics.fmean(data) if data else 0.0

def _safe_std(data, mu=None):
    if len(data) < 2: return 0.0
    return statistics.stdev(data, xbar=mu)

def _percentile(data, p):
    if not data: return 0.0
    data.sort()
    k = (len(data) - 1) * p
    f = math.floor(k)
    c = math.ceil(k)
    if f == c: return data[int(k)]
    d0 = data[int(f)]
    d1 = data[int(c)]
    return d0 + (d1 - d0) * (k - f)

def _downside_deviation(data, target=0.0):
    if not data: return 0.0
    downside_sq = sum((min(x - target, 0.0)) ** 2 for x in data)
    return math.sqrt(downside_sq / len(data))

def _calc_drawdowns(pnls):
    """
    Returns (MaxDD Bps, MaxDD Duration in Trades, Equity Curve)
    """
    if not pnls: return 0.0, 0, []
    
    equity = [0.0]
    curr = 0.0
    for p in pnls:
        curr += p
        equity.append(curr)
        
    peak = -1e9
    max_dd = 0.0
    max_dur = 0
    curr_dur = 0
    
    for val in equity:
        if val > peak:
            peak = val
            curr_dur = 0
        else:
            dd = peak - val
            curr_dur += 1
            if dd > max_dd: max_dd = dd
            if curr_dur > max_dur: max_dur = curr_dur
            
    return max_dd, max_dur, equity

def _calc_streaks(pnls):
    """
    Returns (Max Win Streak, Max Loss Streak)
    """
    if not pnls: return 0, 0
    
    # 1 = Win, -1 = Loss
    signs = [1 if p > 0 else -1 for p in pnls if p != 0]
    if not signs: return 0, 0

    max_win = 0
    max_loss = 0
    
    for k, g in groupby(signs):
        length = len(list(g))
        if k == 1:
            max_win = max(max_win, length)
        else:
            max_loss = max(max_loss, length)
            
    return max_win, max_loss

def generate_scorecard(trades: list[dict]):
    """
    Ingests a list of trade dictionaries.
    Returns the Master Scorecard (Dictionary).
    """
    # Initialize Master Structure (Subset of the prompt's massive list)
    sc = {
        "strategy_returns": {},
        "trade_level": {},
        "regime_conditional": {},
        "robustness_overfitting": {},
        "advanced_structure": {}
    }
    
    if not trades:
        return sc

    # --- Pre-Process Data ---
    pnls = [t['net_pnl_bps'] for t in trades]
    wins = [p for p in pnls if p > 0]
    losses = [p for p in pnls if p <= 0]
    
    # Time Analysis
    # Assumes t['entry_ts'] and t['exit_ts'] are timestamps in ms
    holds_sec = [(t['exit_ts'] - t['entry_ts']) / 1000.0 for t in trades]
    
    # --- 1. Strategy Returns ---
    n = len(pnls)
    mean_bps = _safe_mean(pnls)
    std_bps = _safe_std(pnls, mean_bps)
    gross_pnl = sum(pnls)
    
    max_dd_bps, max_dd_dur, equity_curve = _calc_drawdowns(pnls)
    
    # Annualized approximations (Assuming crypto 24/7, ~Trades/Day estimate needed)
    # We estimate trades per day from data span
    t_start = trades[0]['entry_ts']
    t_end = trades[-1]['exit_ts']
    days = max((t_end - t_start) / 86_400_000.0, 1.0)
    trades_per_day = n / days
    
    # Annualized Vol (Bps) -> StdDev * Sqrt(Trades/Year)
    ann_vol_bps = std_bps * math.sqrt(trades_per_day * 365)
    # Annualized Return (Bps) -> Mean * Trades/Year
    ann_ret_bps = mean_bps * trades_per_day * 365
    
    # Ratios
    sharpe = (ann_ret_bps / ann_vol_bps) if ann_vol_bps > 0 else 0.0
    
    # Sortino (Downside Dev)
    down_dev = _downside_deviation(pnls)
    ann_down_dev = down_dev * math.sqrt(trades_per_day * 365)
    sortino = (ann_ret_bps / ann_down_dev) if ann_down_dev > 0 else 0.0
    
    calmar = (ann_ret_bps / max_dd_bps) if max_dd_bps > 0 else 0.0
    
    sc["strategy_returns"] = {
        "total_trades": n,
        "net_pnl_bps": gross_pnl,
        "gross_mean_return_bps_per_trade": mean_bps,
        "return_vol_bps_per_trade": std_bps,
        "sharpe_annualized": sharpe,
        "sortino_ratio": sortino,
        "calmar_ratio": calmar,
        "max_drawdown_bps": max_dd_bps,
        "max_drawdown_duration_trades": max_dd_dur,
        "recovery_factor": (gross_pnl / max_dd_bps) if max_dd_bps > 0 else 0.0,
        "pnl_skewness": (statistics.mean([(x - mean_bps)**3 for x in pnls]) / (std_bps**3)) if std_bps > 0 else 0.0,
        "pnl_percentile_1": _percentile(pnls, 0.01),
        "pnl_percentile_5": _percentile(pnls, 0.05),
        "pnl_percentile_95": _percentile(pnls, 0.95),
        "pnl_percentile_99": _percentile(pnls, 0.99),
    }

    # --- 2. Trade Level ---
    n_wins = len(wins)
    n_loss = len(losses)
    win_rate = (n_wins / n) * 100.0
    avg_win = _safe_mean(wins)
    avg_loss = _safe_mean(losses)
    
    profit_factor = (sum(wins) / abs(sum(losses))) if sum(losses) != 0 else 0.0
    sqn = (math.sqrt(n) * (mean_bps / std_bps)) if std_bps > 0 else 0.0
    
    streak_win, streak_loss = _calc_streaks(pnls)
    
    sc["trade_level"] = {
        "win_rate_pct": win_rate,
        "avg_win_bps": avg_win,
        "avg_loss_bps": avg_loss,
        "profit_factor": profit_factor,
        "risk_reward_ratio": (avg_win / abs(avg_loss)) if avg_loss != 0 else 0.0,
        "sqn": sqn,
        "trades_per_day": trades_per_day,
        "avg_holding_sec": _safe_mean(holds_sec),
        "max_consecutive_wins": streak_win,
        "max_consecutive_losses": streak_loss,
    }

    # --- 3. Regime Conditional (Kernel Analysis) ---
    # We group by 'k3' (Volatility/Toxicity) and 'k1' (Momentum) if available
    
    # Discretize K3 (Vol) into Low/Mid/High
    # K3 roughly: < -1 (Low Vol), -1 to 1 (Mid), > 1 (High Vol/Tox)
    k3_buckets = {"low_vol": [], "mid_vol": [], "high_vol": []}
    
    for t in trades:
        k3 = t.get('k3', 0.0)
        p = t['net_pnl_bps']
        if k3 < -0.5: k3_buckets["low_vol"].append(p)
        elif k3 > 0.5: k3_buckets["high_vol"].append(p)
        else: k3_buckets["mid_vol"].append(p)
        
    sc["regime_conditional"] = {
        "avg_pnl_low_vol": _safe_mean(k3_buckets["low_vol"]),
        "avg_pnl_mid_vol": _safe_mean(k3_buckets["mid_vol"]),
        "avg_pnl_high_vol": _safe_mean(k3_buckets["high_vol"]),
        "count_low_vol": len(k3_buckets["low_vol"]),
        "count_high_vol": len(k3_buckets["high_vol"]),
    }

    # --- 4. Robustness (Split Half Test) ---
    mid = n // 2
    first_half = pnls[:mid]
    second_half = pnls[mid:]
    
    m1 = _safe_mean(first_half)
    s1 = _safe_std(first_half)
    sh1 = m1 / s1 if s1 > 0 else 0
    
    m2 = _safe_mean(second_half)
    s2 = _safe_std(second_half)
    sh2 = m2 / s2 if s2 > 0 else 0
    
    sc["robustness_overfitting"] = {
        "sharpe_is_first_half": sh1 * math.sqrt(trades_per_day*365),
        "sharpe_oos_second_half": sh2 * math.sqrt(trades_per_day*365),
        "consistency_ratio": min(sh1, sh2) / max(sh1, sh2) if max(sh1, sh2) != 0 else 0.0
    }

    return sc
```

// --- End File: metrics.py ---

// --- File: quick.py ---

```python
"""
quick.py
Rapid Verification - Auto-Latest (Data-Safe, Ordered, 24 contiguous shards).

Automatically finds the newest 6 months of data and runs a simulation,
reusing the same sharding + worker logic as backtest.py (stateful, contiguous).

Design:
- Jobs (days) are sorted chronologically across target months.
- Split into exactly 24 contiguous shards.
- Each shard is processed by backtest.worker_process_shard.
"""

import sys, threading
from concurrent.futures import ThreadPoolExecutor

# Mandatory 3.14t boilerplate
if sys._is_gil_enabled():
    print("!!! WARNING: GIL is ENABLED. Run with: python -X gil=0 quick.py")

CPU_THREADS = 24

import time
import os
from typing import List, Tuple

import config
import backtest
import metrics


def get_latest_months(symbol: str, n: int = 6) -> List[Tuple[int, int]]:
    """
    Scans the data directory to find the most recent N (year, month) folders
    that actually have an index.quantdev present.
    """
    base = os.path.join(config.BASE_DIR, symbol)
    if not os.path.exists(base):
        return []

    try:
        years = [d for d in os.listdir(base) if d.isdigit()]
        years.sort(key=int, reverse=True)  # 2025, 2024, ...
    except Exception:
        return []

    results: List[Tuple[int, int]] = []
    for y_str in years:
        y_int = int(y_str)
        y_path = os.path.join(base, y_str)
        try:
            months = [d for d in os.listdir(y_path) if d.isdigit()]
            months.sort(key=int, reverse=True)  # 12, 11, ...
        except Exception:
            continue

        for m_str in months:
            m_int = int(m_str)
            base_path = os.path.join(y_path, m_str)
            idx_path = os.path.join(base_path, "index.quantdev")
            dat_path = os.path.join(base_path, "data.quantdev")
            if os.path.exists(idx_path) and os.path.exists(dat_path):
                results.append((y_int, m_int))
                if len(results) >= n:
                    return results

    return results


def _scan_jobs_for_months(symbol: str, target_months: List[Tuple[int, int]]):
    """
    Data-safe scan for jobs in target months, fully ordered by date.
    """
    jobs: List[Tuple[str, int, int, str]] = []

    for (year, month) in target_months:
        base_path = os.path.join(config.BASE_DIR, symbol, f"{year:04d}", f"{month:02d}")
        index_path = os.path.join(base_path, "index.quantdev")
        data_path = os.path.join(base_path, "data.quantdev")

        if not os.path.exists(index_path) or not os.path.exists(data_path):
            continue

        try:
            with open(index_path, "rb") as f:
                idx_bytes = f.read()

            month_jobs = []
            ptr = 0
            limit = len(idx_bytes)
            while ptr < limit:
                day, off, ln = config.INDEX_ROW_STRUCT.unpack_from(idx_bytes, ptr)
                ptr += config.INDEX_ROW_SIZE
                month_jobs.append((day, off, ln))

            # CRITICAL: sort by Day (not by write order)
            month_jobs.sort(key=lambda x: x[0])

            for day, off, ln in month_jobs:
                jobs.append(
                    (
                        data_path,
                        off,
                        ln,
                        f"{year:04d}-{month:02d}-{day:02d}",
                    )
                )
        except Exception as e:
            print(f"[WARN] Failed to read index {index_path}: {e}")
            continue

    # Enforce chronological order across all months by date string
    jobs.sort(key=lambda j: j[3])
    return jobs


def _split_jobs_24_contiguous(jobs: List[Tuple[str, int, int, str]]):
    """
    Split jobs into exactly 24 contiguous shards.
    """
    n = len(jobs)
    if n == 0:
        return [[] for _ in range(CPU_THREADS)]

    base = n // CPU_THREADS
    rem = n % CPU_THREADS

    shards: List[List[Tuple[str, int, int, str]]] = []
    idx = 0
    for i in range(CPU_THREADS):
        size = base + (1 if i < rem else 0)
        shard = jobs[idx: idx + size] if size > 0 else []
        shards.append(shard)
        idx += size
    return shards


def run_latest_6_months():
    symbol = config.SYMBOL

    # 1. DISCOVERY
    target_months = get_latest_months(symbol, n=6)
    if not target_months:
        print(f"[Fail] No data folders found for {symbol}")
        return

    # Sorted chronological for reporting
    target_months.sort()
    start_str = f"{target_months[0][0]:04d}-{target_months[0][1]:02d}"
    end_str = f"{target_months[-1][0]:04d}-{target_months[-1][1]:02d}"

    print(f"--- QUICK CHECK: {symbol} [Last 6 Months] ---")
    print(f"Range: {start_str} to {end_str}")
    print(f"Cores: {config.WORKERS} (Ryzen 7900X)")

    # 2. JOB SCANNING (DATA-SAFE)
    print(f"[1/4] Scanning indices for last 6 months...")

    jobs = _scan_jobs_for_months(symbol, target_months)
    if not jobs:
        print("[Fail] Indices found but no jobs. Run data.py.")
        return

    workers = config.WORKERS
    if workers != CPU_THREADS:
        print(f"[WARN] config.WORKERS={workers} but CPU_THREADS={CPU_THREADS}")

    # 3. SHARDING: 24 contiguous shards
    shards = _split_jobs_24_contiguous(jobs)
    print(f"[2/4] Sharded {len(jobs)} chunks across 24 contiguous shards.")

    # 4. PARALLEL EXECUTION (reuse backtest.worker_process_shard)
    print(f"[3/4] Executing Adaptive Multi-Kernel Engine...")
    t0 = time.perf_counter()
    all_trades = []

    with ThreadPoolExecutor(max_workers=workers) as pool:
        futures = [
            pool.submit(backtest.worker_process_shard, i, shards[i], symbol)
            for i in range(CPU_THREADS)
            if shards[i]
        ]
        for fut in futures:
            res = fut.result()
            if res:
                all_trades.extend(res)

    elapsed = time.perf_counter() - t0

    # 5. METRICS
    print(f"\n[4/4] Done in {elapsed:.4f}s")
    if elapsed > 0:
        # Rough row estimate: if you want exact, you would need to read AGG_HDR count per chunk.
        rows_est = len(jobs) * 50_000.0
        print(f"Throughput (rough): {rows_est / elapsed:,.0f} rows/sec")

    print("\n--- Strategy Performance (Recent 6 Months) ---")
    print(f"Period: {start_str} -> {end_str}")

    if all_trades:
        rep = metrics.full_report(all_trades)
        core = rep["core"]
        print(f"Trades:     {int(core['total_trades'])}")
        print(f"Net PnL:    {core['net_pnl_bps']:.2f} bps")
        print(f"Avg Trade:  {core['avg_trade_bps']:.2f} bps")
        print(f"Median:     {core['median_trade_bps']:.2f} bps")
        print(f"P95 Trade:  {core['p95_trade_bps']:.2f} bps")
        print(f"Best Trade: {core['best_trade_bps']:.2f} bps")
        print(f"Worst:      {core['worst_trade_bps']:.2f} bps")
        print(f"Std/Trade:  {core['std_trade_bps']:.2f} bps")
        print(f"Sharpe:     {core['trade_sharpe']:.2f}")
        print(f"Win Rate:   {core['win_rate_pct']:.2f}%")
        print(f"Avg Hold:   {core['avg_hold_sec']:.2f} s")
        print(f"Med Hold:   {core['median_hold_sec']:.2f} s")

        print("\n[Kernel Contribution]")
        for k, v in rep["by_kernel"].items():
            print(
                f"  {k:<12}: net={v['net_pnl_bps']:8.2f} bps, "
                f"trades={v['total_trades']:.0f}, "
                f"sharpe={v['trade_sharpe']:.2f}"
            )
    else:
        print("[WARN] No trades generated. (Check thresholds in algo.py.)")


if __name__ == "__main__":
    run_latest_6_months()
```

// --- End File: quick.py ---

